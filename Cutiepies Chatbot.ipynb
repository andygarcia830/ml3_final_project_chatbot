{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and environment to use with this notebook:<br>\n",
    "conda create -n ml3_final_project scipy scikit-image statsmodels scikit-learn pandas ipykernel tqdm keras pillow matplotlib pytorch==2.4.1 pytorch-cuda=12.4 torchinfo tensorflow=2.17 accelerate langchain pydub -c pytorch -c nvidia -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install prerequisites uncomment and run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install parler-tts\n",
    "# ! pip install spaces\n",
    "# ! pip install accelerate\n",
    "# ! pip install pydub\n",
    "# ! pip install transformers\n",
    "# ! pip install --upgrade protobuf\n",
    "# ! pip install openai-whisper\n",
    "# !pip install langchain_openai\n",
    "# !pip install langchain_deepseek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup The LLM Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query):\n",
    "\n",
    "    template = \"\"\"Question: {question}\n",
    "\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    # llm =ChatDeepSeek(\n",
    "    #         model=\"deepseek-chat\",\n",
    "    llm =ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            # other params...\n",
    "        )\n",
    "    llm_chain = prompt | llm\n",
    "    question = f\"You are a Doctor Michael's assistant interviewing a patient. Only ask one question at a time if needed.  Ask around 3 questions then tell the patient that you will relay the information to the doctor. Do not give answers as lists, and do not use any markup. Do not use apostrpohes to shorten words. Use word representations for numbers. Conversaion history is:{conversation_history} The patient now said: {query}\"\n",
    "    response = llm_chain.invoke(question).content\n",
    "    interaction = {'patient':question, 'you':response}\n",
    "    conversation_history.append(interaction)\n",
    "    if len(conversation_history) > 7:\n",
    "        del conversation_history[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Whisper Speech To Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/miniconda3/envs/ml3_final_project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-19 16:02:16.951948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-19 16:02:16.971671: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-19 16:02:16.977819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 16:02:16.993030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/andy/miniconda3/envs/ml3_final_project/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "# print(\"Using device\", device)\n",
    "model = whisper.load_model(\"base\")\n",
    "model.to(device)\n",
    "# pipe = pipeline(model=model, device=device , return_timestamps=True)\n",
    "\n",
    "def transcribe(audio):\n",
    "    if audio == None:\n",
    "      #  print('NO DATA')\n",
    "       response = 'NO DATA GIVEN'\n",
    "       return response\n",
    "    try:\n",
    "      text = model.transcribe(audio, language=\"en\")[\"text\"]\n",
    "      if len(text.strip()) > 0:\n",
    "        print(f'TRANSCRIBE {text}')\n",
    "      return text\n",
    "    except:\n",
    "       response = 'ERROR TRANSCRIBING'\n",
    "       return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Parler Text To Speech Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash attention 2 is not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING PARLER TTS MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/miniconda3/envs/ml3_final_project/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'parler_tts.dac_wrapper.modeling_dac.DACModel'> is overwritten by shared audio_encoder config: DACConfig {\n",
      "  \"_name_or_path\": \"ylacombe/dac_44khZ_8kbps\",\n",
      "  \"architectures\": [\n",
      "    \"DACModel\"\n",
      "  ],\n",
      "  \"codebook_size\": 1024,\n",
      "  \"frame_rate\": 86,\n",
      "  \"latent_dim\": 1024,\n",
      "  \"model_bitrate\": 8,\n",
      "  \"model_type\": \"dac_on_the_hub\",\n",
      "  \"num_codebooks\": 9,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-tiny/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 1024,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 14,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": false,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY\n"
     ]
    }
   ],
   "source": [
    "from parler_tts import ParlerTTSStreamer\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, set_seed\n",
    "import numpy as np\n",
    "import spaces\n",
    "import torch\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if device != \"cpu\" else torch.float32\n",
    "\n",
    "modelid_tiny = \"parler-tts/parler-tts-tiny-v1\"\n",
    "print('LOADING PARLER TTS MODEL')\n",
    "tts_model = ParlerTTSForConditionalGeneration.from_pretrained(\n",
    "    modelid_tiny, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelid_tiny)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(modelid_tiny)\n",
    "\n",
    "sampling_rate = tts_model.audio_encoder.config.sampling_rate\n",
    "frame_rate = tts_model.audio_encoder.config.frame_rate\n",
    "description = \"Jenna speaks at an average pace with a calm delivery in a very confined sounding environment with clear audio quality.\"\n",
    "description_tokens = tokenizer(description, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print('READY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = feature_extractor.sampling_rate\n",
    "SEED = 42\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "import re\n",
    "\n",
    "def numpy_to_mp3(audio_array, sampling_rate):\n",
    "    # Normalize audio_array if it's floating-point\n",
    "    if np.issubdtype(audio_array.dtype, np.floating):\n",
    "        max_val = np.max(np.abs(audio_array))\n",
    "        audio_array = (audio_array / max_val) * 32767 # Normalize to 16-bit range\n",
    "        audio_array = audio_array.astype(np.int16)\n",
    "\n",
    "    # Create an audio segment from the numpy array\n",
    "    audio_segment = AudioSegment(\n",
    "        audio_array.tobytes(),\n",
    "        frame_rate=sampling_rate,\n",
    "        sample_width=audio_array.dtype.itemsize,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "    # Export the audio segment to MP3 bytes - use a high bitrate to maximise quality\n",
    "    mp3_io = io.BytesIO()\n",
    "    audio_segment.export(mp3_io, format=\"mp3\", bitrate=\"320k\")\n",
    "\n",
    "    # Get the MP3 bytes\n",
    "    mp3_bytes = mp3_io.getvalue()\n",
    "    mp3_io.close()\n",
    "\n",
    "    return mp3_bytes\n",
    "\n",
    "sampling_rate = tts_model.audio_encoder.config.sampling_rate\n",
    "frame_rate = tts_model.audio_encoder.config.frame_rate\n",
    "\n",
    "def split_text(text):\n",
    "    text = text.replace('(','').replace(')','')\n",
    "    phrases = re.split(r'(\\.\\s+|\\n|\\?\\s+|\\!\\s+|\\:\\s+)', text)\n",
    "    # print(phrases)\n",
    "    reconstructed = [ ''.join(x) for x in zip(phrases[0::2], phrases[1::2])]\n",
    "    reconstructed.append(phrases[-1])\n",
    "    stripped = [x.strip().replace('.','') for x in reconstructed if len(x.strip()) > 1]\n",
    "    # print(reconstructed)\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spaces.GPU\n",
    "def read_response(answer):\n",
    "    print(f'READING RESPONSE {answer}')\n",
    "\n",
    "    play_steps_in_s = 3.0\n",
    "    play_steps = int(frame_rate * play_steps_in_s)\n",
    "\n",
    "\n",
    "    phrases = split_text(answer)\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        streamer = ParlerTTSStreamer(tts_model, device=device, play_steps=play_steps)\n",
    "        prompt = tokenizer(phrase, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generation_kwargs = dict(\n",
    "            input_ids=description_tokens.input_ids,\n",
    "            prompt_input_ids=prompt.input_ids,\n",
    "            streamer=streamer,\n",
    "            do_sample=True,\n",
    "            temperature=1.0,\n",
    "            min_new_tokens=10,\n",
    "        )\n",
    "\n",
    "        set_seed(42)\n",
    "        thread = Thread(target=tts_model.generate, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "\n",
    "        for new_audio in streamer:\n",
    "            print(f\"Sample of length: {round(new_audio.shape[0] / sampling_rate, 2)} seconds\")\n",
    "            yield phrase, numpy_to_mp3(new_audio, sampling_rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the Web Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as block:\n",
    "    gr.HTML(\n",
    "        f\"\"\"\n",
    "        <h1 style='text-align: center;'>Cutiepies Healthcare Assistant Chatbot</h1>\n",
    "        <h3 style='text-align: center;'> Ask a question !</h3>\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            audio_out = gr.Audio(label=\"Spoken Answer\", streaming=True, autoplay=True)\n",
    "            query = gr.Textbox(label=\"Query\")\n",
    "            answer = gr.Textbox(label=\"Answer\")\n",
    "            state = gr.State()\n",
    "        with gr.Row():\n",
    "            audio_in = gr.Audio(label=\"Speak your question\", sources=\"microphone\", streaming=False, type=\"filepath\")\n",
    "\n",
    "    audio_in.stop_recording(transcribe, inputs = audio_in, outputs = query)\\\n",
    "            .then(get_response, inputs = query, outputs = answer)\\\n",
    "            .then(fn=read_response, inputs=answer, outputs=[answer, audio_out])\n",
    "\n",
    "\n",
    "\n",
    "block.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSCRIBE  Hello, it's a doctor in right now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING RESPONSE Hello! Doctor Michael is not available at the moment, but I am here to assist you. Can you please tell me what brings you in today?\n",
      "Sample of length: 1.39 seconds\n",
      "Sample of length: 2.41 seconds\n",
      "Sample of length: 1.11 seconds\n",
      "Sample of length: 2.64 seconds\n",
      "TRANSCRIBE  Why is your voice so strange? Anyway, I've had a handicap for around five days now. It's just a handicap. No other symptoms.\n",
      "READING RESPONSE I am sorry if my voice seems strange. I am here to help gather information for Doctor Michael. Can you describe what you mean by a handicap?\n",
      "Sample of length: 2.1 seconds\n",
      "Sample of length: 2.41 seconds\n",
      "Sample of length: 0.64 seconds\n",
      "Sample of length: 2.39 seconds\n",
      "TRANSCRIBE  Sorry, I meant headache.\n",
      "READING RESPONSE Thank you for clarifying. Can you tell me where exactly you feel the headache and if it is constant or comes and goes?\n",
      "Sample of length: 1.85 seconds\n",
      "Sample of length: 2.41 seconds\n",
      "Sample of length: 1.97 seconds\n",
      "TRANSCRIBE  I feel the headache in my head and it's kind of constant. It started when I woke up.\n",
      "READING RESPONSE Thank you for sharing that. Can you tell me if you have noticed anything that makes the headache better or worse?\n",
      "Sample of length: 1.04 seconds\n",
      "Sample of length: 2.41 seconds\n",
      "Sample of length: 1.52 seconds\n",
      "TRANSCRIBE  I try taking parasitamol but no effect.\n",
      "READING RESPONSE Thank you for letting me know. I will relay this information to Doctor Michael, and he will be in touch with you shortly.\n",
      "Sample of length: 1.52 seconds\n",
      "Sample of length: 2.41 seconds\n",
      "Sample of length: 2.66 seconds\n"
     ]
    }
   ],
   "source": [
    "# resp = read_response('Hello, My Name Is Andy')\n",
    "\n",
    "# for item in resp:\n",
    "#     print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
